{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5889d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-core langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7eb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # Required to enable tracing\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # Optional (defaults to this)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Chatbot with LangChain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b96fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! It's great to meet you! Welcome to the world of AI-powered conversations. I'm here to help answer your questions, provide information, and have some fun chats with you. What brings you joy today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model  = Ollama(model=\"llama3\")\n",
    "response = model.invoke(\"Hello, world!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b2635",
   "metadata": {},
   "source": [
    "ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38bd0958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Robot: Greetings human, Bob. Nice to meet you. I'm a language model robot, here to assist and chat with you. What brings you here today?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5874780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! Since I'm an AI and we just started our conversation, I don't have any prior knowledge about you. Could you please tell me your name?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060bee3",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\n",
    "\n",
    "To get around this, we need to pass the entire conversation history into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad667c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob, your name is... (drumroll please)... Bob!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d5f42",
   "metadata": {},
   "source": [
    "LangGraph implements persistence layer that supports multiple conversational turns. \n",
    "\n",
    "So wrapping our chat model in a LangGraph application allows us to automatically persist the message history.\n",
    "\n",
    "LangGraph works as in-memory checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf144e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104aa66",
   "metadata": {},
   "source": [
    "A config has to be passed into the runnable every time. Including a thread_id allows to support multiple conversatinoal threads with a single application, enabling multiple users to use the same application.\n",
    "\n",
    "If we change thread_id, a fresh conversation will start. However, we can always go back to the original conversation (since we are persisting it in a database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "696d25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fe2166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Nice to meet you, Bob! I'm AI, nice and friendly, always happy to chat. What brings you here today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "346b2d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Nice to meet you too, AI!\n",
      "\n",
      "Hmm, that's an interesting question... You are... Bob! (According to the conversation so far)\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97be94a",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b64966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "276f36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# UPDATED CALL MODEL FUNCTION\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state) # Create the prompt\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485207db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Arrrr, shiver me timbers! 'Tis an honor to meet ye, matey Jim! Me name be Captain Blackbeak Billy, and I'll do me best to answer yer questions and swab the decks with ye in no time! What be on yer mind, matey?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8de5341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Arrrr, that be a fine question, matey Jim! Ye asked what yer name be, eh? Well, I be thinkin' it be... (pauses to study the horizon) ...Jim, ain't ye right?! Aye, ye be Jim, and don't ye worry, Captain Blackbeak Billy be here to keep an eye on ye and make sure ye stay shipshape! Now, what be next on yer mind, matey?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6d166",
   "metadata": {},
   "source": [
    "Lets add a new language input to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b8b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "950fcafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Arrr, shiver me timbers! 'Tis a pleasure to make yer acquaintance, Bob! What be bringin' ye to these fair waters? Got a question or just lookin' to pass the time with a swashbucklin' conversation, matey?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language}, # pass the language to the state\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769117d5",
   "metadata": {},
   "source": [
    "Since the state is persisted we can omit parameters like language if no changes are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d5afbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Arrrr, shiver me timbers! Ye be askin' yer own name, eh Bob? Well, matey, I reckon that's a mighty fine question ye got there. And the answer be... (dramatic pause) ...Bob! Aye, ye be right, Bob be yer name! Now, what be bringin' ye to these fair waters, savvy?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a963bd9",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "To improve user experience most application stream back each token as it is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d619b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
